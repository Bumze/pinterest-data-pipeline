{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b676fcb4-59a7-4ba4-b56a-e8779b8451b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "# Import URL processing\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d69036e8-aa6d-4f5d-be2c-7f47787e9cd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check tables in filestore to get name of credentials file\n",
    "dbutils.fs.ls(\"/FileStore/tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e11face-d7f8-4a54-95ae-4b6ee4c4e86d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Read the csv file containing the AWS keys to Databricks \n",
    "# specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to Spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f009bd30-9b4d-4bc2-af09-5bc0294741d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract the access key and secret access key from the spark dataframe created. The secret access key will be encoded using urllib.parse.quote for security purposes. safe=\"\" means that every character will be encoded.\"\"\"\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb22adea-1b61-4ed3-b004-f91e335f256a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount the S3 bucket by passing in the S3 URL and the desired mount name to dbutils.fs.mount().\n",
    "# AWS S3 bucket name\n",
    "AWS_S3_BUCKET = \"user-0e35b2767ae1-bucket\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/user-0e35b2767ae1-bucket\"\n",
    "# Source url\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "# Mount the drive\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805438cd-a18c-4f31-90c8-8dc9c7ad082d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# To check if the S3 bucket was mounted succesfully run the following command:\n",
    "display(dbutils.fs.ls(\"/mnt/user-0e35b2767ae1-bucket/topics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "170b6799-3fc0-4039-af6c-71d70e1c9d2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "topics =  {'df_pin':'0e35b2767ae1.pin', 'df_geo':'0e35b2767ae1.geo', 'df_user':'0e35b2767ae1.user'}\n",
    "for df, topic in topics.items():\n",
    "    file_location = f\"/mnt/user-0e35b2767ae1-bucket/topics/{topic}/partition=0/*.json\" \n",
    "    file_type = \"json\"\n",
    "    # Spark infers the schema\n",
    "    infer_schema = \"true\"\n",
    "    # Read in JSONs from mounted S3 bucket\n",
    "    df_name = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .load(file_location)\n",
    "    # Display Spark dataframe to check its content\n",
    "    display(df_pin)\n",
    "    display(df_geo)\n",
    "    display(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a5b1b0-29e2-4d97-82c8-7b1f5578861e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Unmount S3 bucket\n",
    "# If you want to unmount the S3 bucket, run the following code:\n",
    "\n",
    "dbutils.fs.unmount(\"/mnt/mount_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a967667-adbc-4101-952e-1a6602816e99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Cleaning the dataframes and sorting columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2bd22a6-e79a-40cc-bb7f-fcad5c991d74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning the dataframes and sorting columns\n",
    "\n",
    "df_pin = df_pin.dropDuplicates()\n",
    "df_pin = df_pin.withColumnRenamed('index', 'ind') \n",
    "# Create function to convert null or bad values to None\n",
    "def convert_to_null(df, column, bad_values):\n",
    "    '''Converts no or bad values in dataframe columns to null '''\n",
    "    df = df.withColumn(column, when(col(column).like(bad_values), None).otherwise(col(column)))\n",
    "    return df\n",
    "# Create a dictionary of columns and values to be replaced, then replace them with None in the df\n",
    "bad_values_dict = {\n",
    "    \"description\": \"No description available%\",\n",
    "    \"follower_count\": \"User Info Error\",\n",
    "    \"image_src\": \"Image src error.\",\n",
    "    \"poster_name\": \"User Info Error\",\n",
    "    \"tag_list\": \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\",\n",
    "    \"title\": \"No Title Data Available\"\n",
    "}\n",
    "# Call the function while iterating through the dictionary items.\n",
    "for key, value in bad_values_dict.items():\n",
    "    df_pin = convert_to_null(df_pin, key, value)\n",
    "# Convert non numeric data in the follower_count column to numbers\n",
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\"))\n",
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(\"follower_count\", \"M\", \"000000\"))\n",
    "# Cast all columns with numbers only to integer type\n",
    "df_pin = df_pin.withColumn(\"follower_count\", col(\"follower_count\").cast('int'))\n",
    "df_pin = df_pin.withColumn('downloaded', df_pin['downloaded'].cast('int'))\n",
    "# Convert save_location column to include only the save location path\n",
    "df_pin = df_pin.withColumn(\"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\"))\n",
    "# Re-order the dataframe columns.\n",
    "df_pin = df_pin.select(\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\", \"downloaded\")\n",
    "df_pin.na.drop(how = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5e4d09-da5e-40c9-ba10-04aa339f829a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Clean df_geo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3036dd2c-a689-4d95-8236-25bb4907b8ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df_geo = df_geo.dropDuplicates()\n",
    "# Form an array column from latitude and longitude columns and drop them from the DataFrame\n",
    "df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "df_geo = df_geo.drop('latitude', 'longitude')\n",
    "# Cast column timestamp to Timestamp type\n",
    "df_geo = df_geo.withColumn(\"timestamp\", df_geo[\"timestamp\"].cast('Timestamp'))\n",
    "# Re-order the dataframe columns.\n",
    "df_geo = df_geo.select(\"ind\", \"country\", \"coordinates\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70dc383-4f8d-4825-aedf-69ae5d4945a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Clean df_user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949fd866-e696-4516-a8cc-1954680dd4b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_user = df_user.dropDuplicates()\n",
    "df_user = df_user.withColumn(\"user_name\", concat_ws(' ', (\"first_name\"), (\"last_name\")))\n",
    "df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "df_user = df_user.withColumn('date_joined', df_user['date_joined'].cast('Timestamp'))\n",
    "df_user = df_user.select(\"ind\", \"user_name\", \"age\", \"date_joined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "671bf0c3-2de8-481d-9bb8-3e42a0ed081d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Querying the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14468142-3892-413a-bbac-bbc4237b0850",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Find the most popular Pinterest category people post to based on their country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8faf85e-0498-493f-b6fe-919a7bd60a67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join the pin and geo df on ind, group by columns, count all rows. \n",
    "df_pin.join(df_geo, 'ind').groupBy('country', 'category')\\\n",
    " .agg(count('*').alias('count'))\\\n",
    " .groupBy('country')\\\n",
    " .agg(max(struct('count', 'category')).alias('max_count')) \\\n",
    " .select('country','max_count.category','max_count.count' ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fb7ce57-f96e-43f1-85ce-a506f3c4b28b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell shows categories according to their popularity in all country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9fb737-e5e4-47ae-aea8-2ac61f7b3a0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_pin.join(df_geo, 'ind').groupBy('country', 'category')\\\n",
    "    .agg(count('*').alias('category_count')) \\\n",
    "        .groupBy('country', 'category')\\\n",
    "        .agg(max('category_count').alias('max_category_count')) \\\n",
    "        .select ('country', 'category','max_category_count')\\\n",
    "        .sort(desc(\"max_category_count\")).show()\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23faef1c-ddb5-4bb2-b9e2-d4f091531f5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    " Find how many posts each category had between 2018 and 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91a563e6-39a7-46d1-8c8f-b0d4d6d42cb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Join df_pin with df_geo on 'ind' where year is within given timestamp range\n",
    "df_category_by_specific_years = df_pin.join(df_geo, on='ind') \\\n",
    "    .where((year('timestamp') >= 2018) & (year('timestamp') <= 2022))\\\n",
    "    .groupBy(year('timestamp').alias('post_year'), 'category') \\\n",
    "    .agg(count('*').alias('category_count'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b89078-c03b-40bd-9a42-a3ef5ed9a514",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join df_pin with df_geo on 'ind' where year is within given timestamp range\n",
    "df_category_by_specific_years = df_pin.join(df_geo, on='ind')\\\n",
    "    .where((year('timestamp') >= 2018) & (year('timestamp') <= 2022))\\\n",
    "    .groupBy(year('timestamp').alias('post_year'), 'category')\\\n",
    "    .agg(count('*').alias('post_count'))\\\n",
    "    .groupBy('category') \\\n",
    "    .agg(sum('post_count'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4920d7df-59de-44a4-97f7-ba81f25aa7c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For each country find the user with the most followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "912e92cc-4172-4a89-a2fc-ef726bd0f510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the country with the user that has the most follower\n",
    "# Step 1\n",
    "df_most_followers_by_country = df_pin.join(df_geo, 'ind')\\\n",
    " .groupBy('country')\\\n",
    " .agg(max(struct('follower_count', 'poster_name')).alias('max'))\\\n",
    " .selectExpr('country', 'max.poster_name', 'max.follower_count')\\\n",
    " .orderBy(col('follower_count').desc())\n",
    "df_most_followers_by_country.show()\n",
    "# Step 2\n",
    "df_pin.join(df_geo, 'ind').groupBy('country').agg(max(struct('follower_count', 'poster_name')).alias('max')) \\\n",
    " .selectExpr('country', 'max.poster_name', 'max.follower_count').orderBy(col('follower_count').desc()).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c28a111-512f-41b1-ae25-3790efa59b9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "7. What is the most popular category people post to based on the following age groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece06d4c-0c21-4072-b1d9-0552361f8495",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_max_popular_cat_posted_to_by_age_group = df_pin.join(df_user, 'ind').withColumn('age_group', expr(\"\"\"case\n",
    " WHEN age BETWEEN 18 AND 24 THEN '18-24'\n",
    " WHEN age BETWEEN 25 AND 35 THEN '25-35'\n",
    " WHEN age BETWEEN 36 AND 50 THEN '36-50'\n",
    " WHEN age> 50 THEN '+50' END \"\"\"))\\\n",
    ".groupBy('age_group', 'category') \\\n",
    ".agg(count('*').alias('count')) \\\n",
    ".groupBy('age_group') \\\n",
    ".agg(max(struct('count', 'category')).alias('max_count')) \\\n",
    ".select('age_group', 'max_count.category', 'max_count.count') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3aaa265-2847-45cc-b2b7-39d2f29c3612",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "8. The median follower count for users in the following age groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f5f58f-f6dc-4695-9d03-1d893fcc5938",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_median_followercount_by_age_group = df_pin.join(df_user, 'ind')\\\n",
    "    .withColumn('age_group', expr(\"\"\"case\\\n",
    " WHEN age BETWEEN 18 AND 24 THEN '18-24'\n",
    " WHEN age BETWEEN 25 AND 35 THEN '25-35'\n",
    " WHEN age BETWEEN 36 AND 50 THEN '36-50'\n",
    " WHEN age> 50 THEN '+50' END \"\"\"))\\\n",
    "    .groupBy(\"age_group\") \\\n",
    "    .agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\")) \\\n",
    "    .select(\"age_group\", \"median_follower_count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9199c33-8214-46db-a2a7-d1d15228a929",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "9. Find how many users have joined between 2015 and 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cad32b8-f721-47fa-b28d-56feafa65b2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# Join df_pin with df_geo on 'ind' where year is within given timestamp range\n",
    "df_users_joined_by_years = df_geo.join(df_user, on='ind') \\\n",
    "    .where((year('timestamp') >= 2015) & (year('timestamp') <= 2020))\\\n",
    "    .groupBy(year('timestamp').alias('post_year'))\\\n",
    "    .agg(count('*').alias('number_users_joined'))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f341537-4c5c-4e1a-9d48-868334cdfd10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Find the median follower count of users have joined between 2015 and 2020.\n",
    "\n",
    "post_year, a new column that contains only the year from the timestamp column\n",
    "median_follower_count, a new column containing the desired query output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fce4839f-e6aa-427b-bba6-5338c3f68bf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_median_followercount_by_years = df_user.join(df_pin, 'ind')\\\n",
    "    .groupBy(year('date_joined').alias('post_year'))\\\n",
    "    .agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\")) \\\n",
    "    .select('post_year', 'median_follower_count')\\\n",
    "    .where(col('post_year').between('2015', '2020')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1d44d79-0411-4197-9941-731c81f47bc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Find the median follower count of users that have joined between 2015 and 2020, based on which age group they are part of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48439221-691e-44c9-a6e5-3f81b4a7a76a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_user.join(df_pin, 'ind')\\\n",
    "    .withColumn('age_group', expr(\"\"\"case\\\n",
    " WHEN age BETWEEN 18 AND 24 THEN '18-24'\n",
    " WHEN age BETWEEN 25 AND 35 THEN '25-35'\n",
    " WHEN age BETWEEN 36 AND 50 THEN '36-50'\n",
    " WHEN age> 50 THEN '+50' END \"\"\"))\\\n",
    "    .groupBy(year('date_joined').alias('post_year'), 'age_group')\\\n",
    "    .agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\")) \\\n",
    "    .select('post_year','age_group', 'median_follower_count')\\\n",
    "    .where(col('post_year').between('2015', '2020')).orderBy('post_year','age_group').show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks(data_from_S3_and_query)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
